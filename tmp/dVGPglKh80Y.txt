Today we're going to be exploring the new Canopy framework. This is a framework that has been developed by the GenAI team at Pinecone and the idea is essentially to help us build better RAG pipelines without needing to get into all of the details of how to build a RAG pipeline. Because it's very easy to just build a very simple RAG pipeline but it's very hard to build a good one. And it also comes with a lot of nice little features. One that I really like is the ability to just chat within the terminal and see the difference between a RAG output and a non-RAG output so that you can very quickly evaluate how well your RAG pipeline is performing. Now all of this has been wrapped up into a very easy to use framework so let's jump into it and see how we can use it. Okay so we can see the GitHub repo here and yeah this is a short description and we can kind of come to this visual here it gives us a sort of rough idea of kind of what is going on there. And if we come down to here we can see the different components of Canopy. I'm really going to be focusing on the Canopy CLI down here just to show you know how to get started with it. So we're mostly going to be using everything through CLI. Okay and if we come down to the setup here we have okay you can create a virtual environment you can go ahead and do that it's fine. I'm not going to in this case but then what we do want is we want to install the package. So I'm actually just going to copy this and I'm going to come over to my terminal window here. All right so I'm going to pip install I'm just going to add a upgrade flag here and yes I will let that install. I've already installed it so yeah once it has installed we should be able to just run Canopy and we'll get this error message to begin with and that's because we haven't set a few environment variables. But we do know from this that it isn't solved so to deal with this we need to set some environment variables. So we have PyCon API key, PyCon environment, there's also the OpenAI API key as well that we should add into there. So I'm going to go ahead and do that. I'm going to run vim. I'm just going to add it all of these into a some environment variables file. Okay so I'm going to do on Mac so I'm going to do export PyCon API key. I'm going to put my API key in there. I'm going to do export PyCon environment and also put that in there and then I'm going to do export OpenAI API key put that in there. So for the PyCon API key and environment we go to app.pycon.io we go to API keys and I'm just going to copy this and I'm going to take out my environment as well so us-west1-gcp and come back over here and I'm just going to put in into this here so you can try and see all my API keys if you like and for the OpenAI API key you want to go to platform.openai.com we go to API keys at the top here and I already created one but I'm going to create a new one. So it's going to be demo2. I create my secret key and again I'm just going to go put it in here. Great so put those in and now I can just go ahead and do that. Now with that done let's try and run Kanopy again and we should get something that looks like this. Now what we can do is create a new index. Now to create a new index you'd run Kanopy new and then you'd have your index name. I'm going to call mine Kanopy 101 but I already actually created Kanopy 101 so I'm just going to call it 101a for now. Okay so I confirm okay and then from there what we want to do is actually add our data to this index. Now let me jump across to a notebook and I'll show you how we can create data in the correct format for Kanopy. Okay so we're going to work through this notebook very quickly there'll be a link to this notebook at the top of the video right now. So we're going to take this data set that I scraped from archive it's just a load of AI archive papers. I've used either this version or the chunk version this example a few times in recent videos but if we just take a quick look at what is in there we see that we basically have okay there's this understanding HTML with large language models as a summary and then we have the content. The content is kind of the bit we care most about. Now the content in there is fairly long and typically what we do to handle that is we we have to chunk it up into smaller parts. So let me just take the length of that okay so yes quite a few characters there. That wouldn't all fit into the context window of a LM or you know it may fit in but the whole 400 archive papers definitely wouldn't and when we are feeding knowledge into an LM we also want to be feeding that knowledge into smaller chunks so that we're not filling that context window so that we don't run into LM recall issues. So to avoid that yeah we use chunking and fortunately that's kind of built into Kanopy so we don't even need to like care about it it's going to be done automatically. All we need to do is set up this data format here so we have id text source so where the source is coming from you don't have to pass that you can just leave it blank it's fine and then metadata which is just a dictionary containing any relevant information that you may or may not want in and like attached to your vectors. Again you don't need to put anything in here okay so we run this this is just transforming our working face data set into this format okay and removing the columns that we don't want. Then what I'm going to do is convert this into a JSON lines file okay and then should be able to take a look at that over here and yeah we can see we can see all of this. Okay so with that done we can move on to actually putting all this into our index using Kanopy. Okay once we have our data set we can go ahead and run Kanopy upsert and it would be in here so this is where I saved my data in the same directory I'm in now and actually you know we can just see that quickly yeah okay so I'm going to upsert this so Kanopy upsert there we go. Now when we try and do that we're actually going to get this error and that's because we also need a index name environment variable so we'll go ahead and do that as well. You can also set index name here within the command but I'm going to do it via the via this. Okay and I want a to start with and do the upsert it'll ask us to confirm that everything was correct so just you know quick check it you know looks pretty good say yes and we continue and then yeah we're going to get this loading bar it's going to just show us the progress of our upsert but I've already created my index doing this exact same process so I'm going to actually cancel that and what I'm going to do is change my index name to that other index and then I'm going to start Kanopy okay so I'm going to do Kanopy start and what this is going to do is start up the API or Kanopy server okay so from here I can actually you know I could go to my localhost 8000 and go to the docs and I can see if I zoom in a little bit so you have some documentation we have all the endpoints and stuff in here that we can we can use now I actually want to use the CLI now the CLI requires that you have the the Kanopy server running in the background so I'm going to switch across to a new terminal window I'm going to activate my ML environment I'm going to run source magenv and I'm going to export my index name then what I want to do is run Kanopy chat and so you can run Kanopy chat without any arguments and that will that will you know it's like you're chatting with your LLM and it's doing rag in the background and getting your responses but I also actually want to do it with no rag what no rag will do is show us a comparison of the LLM response with and without right so this is incredibly useful for just evaluating what rag is actually doing for you so yeah let's see let's take a look at this and yeah we should see some pretty interesting results okay cool so get a nice little note up there this is debugging tool not to be used for production but that's cool because we're just testing it so hello there start with that press escape and enter I'll send my my query with context rag okay so we see with this query we literally get the same response because you know it doesn't really matter whether we're using rag or not for general chat but what if we ask something you know like an actual query that is relevant to the data set that we put behind this so our data set contains information about LLM2 the large language model because this is a it's an archive data set on like AI so I can ask it something like that I can ask it can you tell me about LLM2 so obviously with context LLM2 is a collection of pre-trained and fine-tuned large language models ranging in scale from 770 billion parameters so on and so on right that's cool then no rag I apologize but I'm not aware of any specific entity called LLM2 okay so this this LLM it just doesn't know anything about LLM2 because its training data cutoff was like September 2021 so yeah it cannot know about that so I don't know let's continue the conversation like okay fascinating can you tell me more about when am I going to use LLM2 okay let's see what we get okay cool so with context rag we have LLM2 specifically the fine-tuned LLMs optimized for dialogue use cases found to outperform open-source chat models on most benchmarks that were tested so on and so on okay so also gives us a source document which is pretty nice now without a context okay LLM2 can serve various purposes and be useful in different situations can use those pack animals therapy animals guard animals apparently I didn't know that and okay maybe and in sustainable agriculture so obviously one of those answers is a little bit better than the other at least for our for our use case now let's ask you a slightly more complicated question so can you tell me about LLM2 versus distilbert now this is the sort of question where a typical rag pipeline if not built well will probably struggle because there's actually kind of two search queries in here we want to be searching for LLM2 and we also want to be searching for distilbert which appear in different papers but typically the way that rag would be implemented at least you know your first versions and whatever else that's probably going to get passed to your vector database as a single query the good thing about canopy is that it will handle this and it will actually split this up into multiple queries so we're doing multiple searches getting results from the distilbert paper and the LLM2 paper and then it's going to provide us hopefully with a good comparison between the two all right so LLM2 is a collection of pre-trained and finding lost language models so on and so on cool distilbert is smaller faster and lighter version of BERT language model in summary LLM2 is specifically summarized for dialogue use cases whilst distilbert is a more efficient version of BERT model that can use the various natural language processing tasks okay i think you know so it's a good comparison without context obviously it doesn't know what LLM2 is so yeah it's like okay it's not a known entity or term in the realm of NLP or AI however distilbert refers to a specific model architecture used for various NLP tasks so it actually the it can tell us a little bit about distilbert because this is a this is an older model so it it does know about that but it can't give us a good comparison so that's a very quick introduction to the canopy framework i think from this you can very clearly see what the pros of using something like this are of course this is just the CLI there's also the canopy server and the actual framework itself i wish you can obviously go ahead and and try out but for now that's it for this video i hope all this has been useful so thank you very much for watching and i will see you again in the next one